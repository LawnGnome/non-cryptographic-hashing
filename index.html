<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Non-cryptographic hashing</title>

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/fonts.css">
		<link rel="stylesheet" href="reveal.js/css/reveal.css">
		<link rel="stylesheet" href="css/sky.css" id="theme">

    <!-- CSS overrides -->
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet/less" type="text/css" href="less/animate.less">
    <link rel="stylesheet/less" type="text/css" href="less/flowchart.less">

    <script src="less.min.js"></script>

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<div class="slides">
				<section class="title">
          <h1>Non-cryptographic hashing</h1>
          <h3>
            Adam Harvey
            <br>
            <a href="https://twitter.com/LGnome">@LGnome</a>
            <br>
            <a href="http://newrelic.com">New Relic</a>
          </h3>
				</section>

        <section>
          <section>
            <h1>What?</h1>
          </section>

          <section>
            <h2>Hash function</h2>
            <aside class="notes">
              Let's start with the obvious: what is a hash function? (Most of
              you likely know this already, but let's make sure we're all on
              the same page.
            </aside>
          </section>

          <section class="input-output">
            <div class="input scroll-4">
              <div>
                <code>Lorem ipsum dolor sit amet,</code>
                <code>consectetur adipiscing elit.</code>
                <code>Integer vitae ex nec nisl</code>
                <code>fermentum sodales in ut lectus.</code>
              </div>
            </div>
            <div style="font-size: 2em">
              <span class="octicon mega-octicon octicon-arrow-down"></span>
            </div>
            <div class="output cycle-4">
              <code>61df8894</code>
              <code>0ec572f8</code>
              <code>19e37640</code>
              <code>372ecf1b</code>
            </div>
            <aside class="notes">
              Fundamentally, a hash function takes any amount of data and
              returns a number based on that data.
            </aside>
          </section>

          <section>
            <h2 id="hash-functions">
              <code>MD5</code>
              <code>SHA-1</code>
              <code>SHA-256</code>
              <code>SHA-512</code>
            </h2>
            <aside class="notes">
              We use them every day. They underpin SSL, block encryption,
              password storage, and many other things that we take for granted.
            </aside>
          </section>

          <section>
            <h2><strong style="opacity: 0">Non-</strong>Cryptographic hash function</h2>
            <aside class="notes">
              All of those are cryptographic hash functions, though. They're
              designed with specific qualities in mind: to make it as difficult
              as possible to generate collisions and to be able to work back to
              find out what the input was. Key derivation functions extend this
              for use with passwords: bcrypt uses the Blowfish block cipher in
              conjunction with a basic hashing function to generate CPU-hard
              hashes based on salted passwords.
            </aside>
          </section>

          <section>
            <h2><strong>Non-</strong>Cryptographic hash function</h2>
            <aside class="notes">
              <p>
                Not all hash functions have to be cryptographic, though.
                There's also a place for functions that are fast and reasonably
                well distributed, but provide no guarantee of security. Some
                can also minimise memory usage.
              </p>
              <p>
                This is the world of the non-cryptographic hash function.
              </p>
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1>Why?</h1>
          </section>

          <section>
            <h2>What have I seen?</h2>
            <aside class="notes">
              Python provides a set type, but when you're dealing with a lot of
              data you may not want to keep everything in memory. Hashing it
              first can help, particularly if you're doing something that can
              withstand the odd false positive.
            </aside>
          </section>

          <section>
            <h2>Is this valid?</h2>
            <aside class="notes">
              While you'd often need to use a cryptographic hash function for
              checksumming, there are scenarios where you really do just want
              the 2010s version of a parity bit. These can be quick and simple.
            </aside>
          </section>

          <section>
            <h2>Interoperability</h2>
            <aside class="notes">
              People spec weird things all the time. New Relic CAT example.
            </aside>
          </section>

          <section>
            <h2>It's fun?</h2>
          </section>
        </section>

        <section>
          <section>
            <h1>How?</h1>
          </section>

          <section>
            <h2>Goals</h2>
            <ul>
              <li>Fast</li>
              <li>Well distributed output</li>
            </ul>
            <aside class="notes">
              <p>
                There are two key goals. We want something that's quick, and we
                want something that distributes its output well. A common use
                of non-cryptographic hash functions is in implementing hash
                tables &mdash; while that's not usually a huge concern in
                Python, you don't want clumps in your output, because that
                often leads to collisions.
              </p>
            </aside>
          </section>

          <section>
            <h2>Avalanche effect</h2>
            <div class="input">
              <div><code>000<strong>10</strong>000</code></div>
              <div><code>000<strong>01</strong>000</code></div>
              <div><code>000<strong>11</strong>000</code></div>
              <div><code>000<strong>00</strong>000</code></div>
            </div>
            <div style="font-size: 2em">
              <span class="octicon mega-octicon octicon-arrow-down"></span>
            </div>
            <div class="output">
              <code>000<strong>??</strong>00</code>
            </div>
            <aside class="notes">
              This is what you don't want. You want each change to the data to
              have a bigger effect on the output.
            </aside>
          </section>

          <section>
            <h2>Avalanche effect</h2>
            <div class="input">
              <div><code>000<strong>10</strong>000</code></div>
              <div><code>000<strong>01</strong>000</code></div>
              <div><code>000<strong>11</strong>000</code></div>
              <div><code>000<strong>00</strong>000</code></div>
            </div>
            <div style="font-size: 2em">
              <span class="octicon mega-octicon octicon-arrow-down"></span>
            </div>
            <div class="output">
              <code><strong>????????</strong></code>
            </div>
            <aside class="notes">
              Having each change have a large change on the output is called
              the avalanche effect.
            </aside>
          </section>

          <section>
            <pre><code class="python" data-trim>
def hash_function(data: bytes) -&gt; int:
  for chunk in data:
    hash = cleverness(hash, chunk)

  return hash
            </code></pre>
            <aside class="notes">
              <p>
                Broadly, this is how every hash function is constructed. Easy,
                huh? (Python provides a cleverness function, I'm sure.)
                Cleverness is actually a "mixing function": it takes the
                current state and mutates it with the chunk.
              </p>
              <p>
                Chunk sizes vary, but are most commonly bytes or 32-bit words.
              </p>
              <p>
                As an aside, cryptographic hash functions usually use a
                Merkle-Damgård (DAM-gour(d)) construction, which adds padding
                in a specific way, but is otherwise a fancy (and hard) way of
                saying this.
              </p>
            </aside>
          </section>

          <section>
            <h2>Cleverness</h2>
            <ul>
              <li>Unsigned, fixed length integers</li>
              <li>Bitwise operations (<code>xor</code>; bitshifts)</li>
              <li>Multiplication</li>
              <li>Primes</li>
            </ul>
            <aside class="notes">
              The basic constructs that tend to be used. Bitshifts and xor are
              extremely useful when the input has little variance, as they can
              be used to force lots of changes to otherwise unaffected bits.
            </aside>
          </section>

          <section>
            <h2>Integers</h2>
            <pre><code>  0xffffffff
+ 0x00000001
= 0x00000000</code></pre>
            <aside class="notes">
              A brief digression into integers. Python very helpfully provides
              unlimited precision integers, but these constructions generally
              rely on integer overflow: specifically, that integers will wrap
              and only the lowest bits will be preserved.
            </aside>
          </section>

          <section>
            <h2>NumPy</h2>
            <pre><code class="python" data-trim>
from numpy import uint32
            </code></pre>
            <aside class="notes">
              On the bright side, NumPy gives us a good uint32 type that
              supports the semantics we need.
            </aside>
          </section>

          <section>
            <code><pre>&gt;&gt;&gt; from numpy import uint32
&gt;&gt;&gt; a = uint32(42)
&gt;&gt;&gt; a *= 1
&gt;&gt;&gt; print(type(a))
&lt;class 'numpy.<strong>int64</strong>'&gt;</pre></code>
            <aside class="notes">
              Unfortunately, it still autopromotes when you give it Python
              integers.
            </aside>
          </section>

          <section>
            <code><pre>&gt;&gt;&gt; from numpy import uint32
&gt;&gt;&gt; a = uint32(42)
&gt;&gt;&gt; a *= <strong>uint32(1)</strong>
&gt;&gt;&gt; print(type(a))
&lt;class 'numpy.<strong>uint32</strong>'&gt;</pre></code>
            <aside class="notes">
              You're going to see some ugly code. I apologise.
            </aside>
          </section>

          <section>
            <h2>Primes</h2>
            <aside class="notes">
              Most hashing algorithms include one or more (usually more) prime
              numbers as constants. This is mostly for bucketing behaviour: if
              you're using the output to place things into buckets, you don't
              want common factors lest you end up bucketing naïvely into the
              same buckets &mdash; if your hash function is biased towards
              multiples of eight and you're implementing something with eight
              buckets, you may not end up using seven of them much.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1>FNV-1a</h1>
            <aside class="notes">
              Let's walk through an example: in this case, FNV-1a. FNV was
              created by Glenn Fowler, Landon Curt Noll and Phong Vo in 1991,
              and later revised to improve its avalanche behaviour. It's still
              widely used today, and is simple enough that we can step through
              it and see the principles noted above in action.
            </aside>
          </section>

          <section>
            <h2>FNV-1a</h2>
            <pre><code class="python" data-trim>
def fnv1a32(data: bytes) -&gt; uint32:
  hash = uint32(2166136261)
  for byte in data:
    hash ^= uint32(byte)
    hash *= uint32(16777619)
  return hash
            </code></pre>
            <aside class="notes">
              Here's the Python implementation of the algorithm. Note the ugly
              uint32() calls, as previously threatened. Conceptually, though,
              it's very simple: for each byte (since it's a bytewise
              algorithm), we xor the hash value, then multiply it.
            </aside>
          </section>

          <!--
          <section>
            <h2>FNV-1a</h2>
            <svg class="flowchart" width="1000" height="500">
              <defs>
                <marker id="arrowhead" markerWidth="6" markerHeight="6" orient="auto" refX="6" refY="3">
                  <path d="M0,0 L6,3 0,6" />
                </marker>
              </defs>

              <rect class="terminal" x="5" y="210" height="80" width="150" rx="20" ry="20" />
              <text class="label" x="80" y="250">data</text>

              <rect class="terminal" x="845" y="210" height="80" width="150" rx="20" ry="20" />
              <text class="label" x="920" y="250">hash</text>

              <rect class="action xor" x="370" y="210" height="80" width="80" />
              <text class="label" x="410" y="250">^</text>

              <rect class="action mult" x="550" y="210" height="80" width="80" />
              <text class="label" x="590" y="250">*</text>

              <line class="arrow" x1="155" y1="250" x2="370" y2="250" />
              <line class="arrow" x1="450" y1="250" x2="550" y2="250" />
              <line class="arrow" x1="630" y1="250" x2="845" y2="250" />
              <path class="arrow" d="M590 290 V 340 C 590 440, 410 440, 410 340 V 290" />
            </svg>
          </section>
          -->

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
&gt;   hash = uint32(2166136261)
    for byte in data:
      hash ^= uint32(byte)
      hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1000 0001 0001 1100 1001 1101 1100 0101</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>(unset)</pre></code>
            <aside class="notes">
              First, we set our hash value to a randomly selected number. This
              isn't actually a prime: it's not chosen for its dispersal
              characteristics, but rather is simply a non-zero value so that
              the initial hash value isn't all zeroes. (In fact, it's the FNV-0
              hash of the e-mail signature line of Landon Curt Noll!)
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
&gt;   for byte in data:
      hash ^= uint32(byte)
      hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1000 0001 0001 1100 1001 1101 1100 0101</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 0001</pre></code>
            <aside class="notes">
              Pick a byte (lowercase "a").
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
    for byte in data:
&gt;     hash ^= uint32(byte)
      hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1000 0001 0001 1100 1001 1101 1<strong>01</strong>0 010<strong>0</strong></pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 0001</pre></code>
            <aside class="notes">
              Now we do the exclusive-or. Note that only three bits in hash
              actually changed. Not very avalanchey!
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
    for byte in data:
      hash ^= uint32(byte)
&gt;     hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1<strong>11</strong>0 0<strong>1</strong>0<strong>0</strong> 000<strong>0</strong> 1100 <strong>0</strong>0<strong>10</strong> 1<strong>0</strong>01 <strong>001</strong>0 <strong>1</strong>100</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 0001</pre></code>
            <aside class="notes">
              Now we see some changes! The sharp of eye would have noticed that
              this prime is close to 2^24, which means that we're effectively
              performing a bitshift with some added noise in the lower bits.
              There's some avalanche.
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
&gt;   for byte in data:
      hash ^= uint32(byte)
      hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1110 0100 0000 1100 0010 1001 0010 1100</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 00<strong>10</strong></pre></code>
            <aside class="notes">
              Now we do it again with b, which is only two bits different from
              a.
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
    for byte in data:
&gt;     hash ^= uint32(byte)
      hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre>1110 0100 0000 1100 0010 1001 0<strong>10</strong>0 11<strong>1</strong>0</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 0001</pre></code>
            <aside class="notes">
              Again, only three bits change when we do the xor.
            </aside>
          </section>

          <section>
            <h3><code>fnv1a32(b"ab")</code></h3>
            <pre><code class="python">  def fnv1a32(data: bytes) -&gt; uint32:
    hash = uint32(2166136261)
    for byte in data:
      hash ^= uint32(byte)
&gt;     hash *= uint32(16777619)
    return hash</code></pre>
            <h4><code>hash</code> (binary):</h4>
            <code><pre><strong>0</strong>1<strong>0</strong>0 <strong>1</strong>10<strong>1</strong> 00<strong>1</strong>0 <strong>0</strong>10<strong>1</strong> 00<strong>0</strong>0 <strong>01</strong>01 <strong>1</strong>100 1<strong>0</strong>10</pre></code>
            <h4><code>byte</code> (binary):</h4>
            <code><pre>0000 0000 0000 0000 0000 0000 0110 0001</pre></code>
            <aside class="notes">
              You can see how well distributed the changes are, even after only
              two bytes of input. Every nibble has one or two bits modified. A
              small change resulted in a big change.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1>Shootout</h1>
            <aside class="notes">
              Let's look at a few functions (specifically, the 32 bit versions
              where appropriate). I've chosen functions that are feasible to
              implement in Python and generally considered to be among the
              state of the art. Firstly, though, we need a way to compare them.
            </aside>
          </section>

          <section>
            <h2>Goals</h2>
            <ul>
              <li>Fast</li>
              <li>Well distributed output</li>
            </ul>
            <aside class="notes">
              Let's look at our goals again. How can we judge them?
            </aside>
          </section>

          <section>
            <h2>Fast</h2>
            <aside class="notes">
              Fast is easy. CPU time pretty much tells us that.
            </aside>
          </section>

          <section>
            <h2>Distribution</h2>
            <aside class="notes">
              We can look at collisions. I'm going to test each algorithm we
              look at against two input sets: OS X's /usr/share/dict/words, and
              the numbers 0-235885 (so it's the same size as the words)
              represented as strings.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1>FNV-1a</h1>
            <aside class="notes">
              Since we've already looked at FNV-1a in some detail, let's look
              at some results for it.
            </aside>
          </section>

          <section>
            <h2>FNV-1a</h2>
            <table>
              <thead>
                <tr>
                  <th>Corpus</th>
                  <th>CPU time (s)</th>
                  <th>Collisions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numbers</td>
                  <td>1.633929</td>
                  <td>0</td>
                </tr>
                <tr>
                  <td>Words</td>
                  <td>2.556861</td>
                  <td>5 (0.002%)</td>
                </tr>
              </tbody>
            </table>
            <aside class="notes">
              There are collisions in the word corpus. This isn't really
              unexpected: we're only getting 32 bit values, and given the
              birthday paradox, there's a greater than 75% chance of a
              collision for a naïve hash function with more than 200000 inputs.
            </aside>
          </section>

          <section>
            <h2>FNV-1a</h2>
            <img src="images/fnv1a32.png">
            <aside class="notes">
              <p>
                Another way to visualise the distribution besides just the
                number of collisions is to map it out. I've shamelessly stolen
                this approach from Ian Boyd, who wrote a seminal StackExchange
                essay on hashing functions three years ago.
              </p>
              <p>
                White dots indicate unused values within the output.
              </p>
              <p>
                Basically, you want this to look like noise. FNV-1a is pretty
                good on this count.
              </p>
            </aside>
          </section>
        </section>

        <section>
          <section id="mm3-title">
            <h1>MurmurHash3</h1>
            <aside class="notes">
              MurmurHash3 is another relatively simple to implement hashing
              function. As the name implies, it's the third revision of a
              function designed by Austin Appleby.
            </aside>
          </section>

          <section id="mm3">
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim style="font-size: 0.37em">
def murmurhash3(data: bytes, seed: uint32) -> uint32:
    c1 = uint32(0xcc9e2d51)
    c2 = uint32(0x1b873593)
    r1 = uint32(15)
    r2 = uint32(13)
    m = uint32(5)
    n = uint32(0xe6546b64)

    hash = see

    i = 0
    while i < len(data):
        c = data[i:i+4]
        i += 4

        if len(c) == 4:
            k = uint32(unpack("=L", c)[0])
            k *= c1
            k = (k << r1) | (k >> (uint32(32) - r1))
            k *= c2
            hash ^= k
            hash = (hash << r2) | (hash >> (uint32(32) - r2))
            hash = hash * m + n
        else:
            k = uint32(0)
            if len(c) == 3:
                k ^= uint32(c[2] << 16)
            if len(c) >= 2:
                k ^= uint32(c[1] << 8)
            if len(c) >= 1:
                k ^= uint32(c[0])
                k *= c1
                k = (k << r1) | (k >> (uint32(32) - r1))
                k *= c2
                hash ^= k

    hash ^= uint32(len(data))
    hash ^= uint32(hash >> 16)
    hash *= uint32(0x85ebca6b)
    hash ^= uint32(hash << 13)
    hash *= uint32(0xc2b2ae35)
    hash ^= uint32(hash >> 16)

    return hash
            </code></pre>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim>
def murmurhash3(data: bytes, seed: uint32) -> uint32:
    c1 = uint32(0xcc9e2d51)
    c2 = uint32(0x1b873593)
    r1 = uint32(15)
    r2 = uint32(13)
    m = uint32(5)
    n = uint32(0xe6546b64)

    hash = seed
            </code></pre>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim>
while i < len(data):
    c = data[i:i+4]
    i += 4
            </code></pre>
            <aside class="notes">
              Grab a four byte chunk. MurmurHash is unusual in that it operates
              on words at a time, not bytes.
            </aside>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim style="font-size: 1.05em">
if len(c) == 4:
    k = uint32(unpack("=L", c)[0])
    k *= c1
    k = (k << r1) | (k >> (uint32(32) - r1))
    k *= c2
    hash ^= k
    hash = (hash << r2) | (hash >> (uint32(32) - r2))
    hash = hash * m + n
            </code></pre>
            <aside class="notes">
              The use of struct.unpack probably seems anachronistic now, but
              it's actually quicker than int.from_bytes.
            </aside>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim style="font-size: 1.15em">
else:
    k = uint32(0)
    if len(c) == 3:
        k ^= uint32(c[2] << 16)
    if len(c) >= 2:
        k ^= uint32(c[1] << 8)
    if len(c) >= 1:
        k ^= uint32(c[0])
        k *= c1
        k = (k << r1) | (k >> (uint32(32) - r1))
        k *= c2
        hash ^= k
            </code></pre>
            <aside class="notes">
              Go through some gymnastics for the last few bytes to ensure
              they're properly mixed into the hash value. The use of multiple
              mixing functions is common in hash functions that deal with more
              than one byte at a time.
            </aside>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <pre><code class="python" data-trim>
hash ^= uint32(len(data))
hash ^= uint32(hash >> 16)
hash *= uint32(0x85ebca6b)
hash ^= uint32(hash << 13)
hash *= uint32(0xc2b2ae35)
hash ^= uint32(hash >> 16)

return hash
            </code></pre>
            <aside class="notes">
              Finally, after all mixing, some postprocessing occurs. This is
              common in newer hash functions: with careful chosen values, you
              can attempt to invoke additional avalanching.
            </aside>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <table>
              <thead>
                <tr>
                  <th>Corpus</th>
                  <th>CPU time (s)</th>
                  <th>Collisions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numbers</td>
                  <td>4.409535</td>
                  <td>5 (0.002%)</td>
                </tr>
                <tr>
                  <td>Words</td>
                  <td>4.965808</td>
                  <td>5 (0.002%)</td>
                </tr>
              </tbody>
            </table>
            <aside class="notes">
              A few more collisions, which is unfortunate. Much more
              importantly, though, it's almost three times slower than FNV-1a.
              This is due to CPython being relatively slow, the large number of
              uint32() objects that have to be instantiatied to prevent
              overflow, and the chunking behaviour. In C, though, none of these
              are issues.
            </aside>
          </section>

          <section>
            <h2>MurmurHash3</h2>
            <img src="images/murmurhash3.png">
          </section>
        </section>

        <section>
          <section>
            <h1>xxHash</h1>
            <aside class="notes">
              xxHash is another relatively new hash function that has been
              quickly adopted by a number of projects, including pfSense and
              TeamViewer. (For what, I don't know.) It has a similar
              construction to MurmurHash3, except it reads 16 bytes at a time.
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <pre><code class="python" data-trim style="font-size: 0.25em">
def xxhash32(data: bytes, seed: uint32) -> uint32:
    primes = [uint32(n) for n in (
        2654435761,
        2246822519,
        3266489917,
        668265263,
        374761393
    )]

    def rotl(x: uint32, r) -> uint32:
        return (x << uint32(r)) | (x >> uint32(32 - r))

    offset = 0
    def get32bits():
        nonlocal offset

        chunk = data[offset:offset+4]
        offset += 4
        return uint32(unpack("=L", chunk)[0])

    hash = uint32(0)

    if len(data) >= 16:
        v1 = seed + primes[0] + primes[1]
        v2 = seed + primes[1]
        v3 = seed
        v4 = seed - primes[0]

        def mix(v: uint32, chunk: uint32) -> uint32:
            v += chunk * primes[1]
            v = rotl(v, 13)
            return v * primes[0]

        while offset <= (len(data) - 16):
            v1 = mix(v1, get32bits())
            v2 = mix(v2, get32bits())
            v3 = mix(v3, get32bits())
            v4 = mix(v4, get32bits())

        hash = rotl(v1, 1) + rotl(v2, 7) + rotl(v3, 12) + rotl(v4, 18)
    else:
        hash = seed + primes[4]

    hash += uint32(len(data))

    while (offset + 4) < len(data):
        hash += get32bits() * primes[2]
        hash = rotl(hash, 17) * primes[3]

    while offset < len(data):
        hash += uint32(data[offset]) * primes[4]
        offset += 1
        hash = rotl(hash, 11) * primes[0]

    hash ^= hash >> uint32(15)
    hash *= primes[1]
    hash ^= hash >> uint32(13)
    hash *= primes[2]
    hash ^= hash >> uint32(16)

    return hash
            </code></pre>
            <aside class="notes">
              This one's even bigger &mdash; probably as big as you'd really
              ever want to implement &mdash; but the principle is the same.
              Some primes are defined atop, some utility functions are defined,
              and then there are three mixing functions: one for when you can
              read 16 bytes, one for 4 bytes, and one for 1 byte.
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <pre><code class="python" data-trim style="font-size: 1.3em">
def mix(v: uint32, chunk: uint32) -> uint32:
    v += chunk * primes[1]
    v = rotl(v, 13)
    return v * primes[0]

while offset <= (len(data) - 16):
    v1 = mix(v1, get32bits())
    v2 = mix(v2, get32bits())
    v3 = mix(v3, get32bits())
    v4 = mix(v4, get32bits())

hash = rotl(v1, 1) + rotl(v2, 7) +
       rotl(v3, 12) + rotl(v4, 18)
            </code></pre>
            <aside class="notes">
              Here's the 16 byte mixing function. Note that it keeps four
              running values and combines them at the end.
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <pre><code class="python" data-trim style="font-size: 1.3em">
while (offset + 4) < len(data):
    hash += get32bits() * primes[2]
    hash = rotl(hash, 17) * primes[3]

while offset < len(data):
    hash += uint32(data[offset]) * primes[4]
    offset += 1
    hash = rotl(hash, 11) * primes[0]
            </code></pre>
            <aside class="notes">
              There are then the 4 and 1 byte mixers, which are used until all
              the data has been mixed.
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <pre><code class="python" data-trim style="font-size: 1.3em">
hash ^= hash >> uint32(15)
hash *= primes[1]
hash ^= hash >> uint32(13)
hash *= primes[2]
hash ^= hash >> uint32(16)

return hash
            </code></pre>
            <aside class="notes">
              Finally, a post processing step. Notice how these all look pretty
              similar now you know how to break them down?
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <table>
              <thead>
                <tr>
                  <th>Corpus</th>
                  <th>CPU time (s)</th>
                  <th>Collisions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numbers</td>
                  <td>3.337822</td>
                  <td>0</td>
                </tr>
                <tr>
                  <td>Words</td>
                  <td>4.032726</td>
                  <td>12 (0.005%)</td>
                </tr>
              </tbody>
            </table>
            <aside class="notes">
              No number collisions, which probably indicates the type of data
              xxHash was most tested with.
            </aside>
          </section>

          <section>
            <h2>xxHash</h2>
            <img src="images/xxhash32.png">
          </section>
        </section>

        <section>
          <section>
            <h1 style="font-size: 2.5em">SuperFastHash</h1>
            <aside class="notes">
              All these good hash functions are boring. Let's look at what
              happens when you use a bad function. To quote NonCryptoHashZoo:
              SuperFastHash is the quintessential example of how easy it is to
              go wrong when making your own hash function even if you're
              incredibly smart.
            </aside>
          </section>

          <section>
            <h2>SuperFastHash</h2>
            <pre><code class="python" data-trim style="font-size: 0.38em">
def superfasthash(data: bytes, seed: uint32) -> uint32:
    hash = seed + uint32(len(data))

    i = 0
    while i < len(data):
        c = data[i:i+4]
        i += 4

        if len(c) == 4:
            high = uint32(unpack("=H", c[0:2])[0])
            low = uint32(unpack("=H", c[2:4])[0])

            hash += high
            tmp = (low << uint32(11)) ^ hash
            hash = (hash << uint32(16)) ^ tmp
            hash += hash >> uint32(11)
        elif len(c) == 3:
            high = uint32(unpack("=H", c[0:2])[0])
            low = uint32(unpack("=B", c[2:])[0])

            hash += high
            hash ^= hash << uint32(16)
            hash ^= low << uint32(18)
            hash += hash >> uint32(11)
        elif len(c) == 2:
            hash += uint32(unpack("=H", c)[0])
            hash ^= hash << uint32(11)
            hash += hash >> uint32(17)
        elif len(c) == 1:
            hash += uint32(unpack("=B", c)[0])
            hash ^= hash << uint32(10)
            hash += hash >> uint32(1)

    hash ^= hash << uint32(3)
    hash += hash >> uint32(5)
    hash ^= hash << uint32(4)
    hash += hash >> uint32(17)
    hash ^= hash << uint32(25)
    hash += hash >> uint32(6)

    return hash
            </code></pre>
            <aside class="notes">
              Interesting: there's no initial state besides the seed and the
              length of the input.
            </aside>
          </section>

          <section>
            <h2>SuperFastHash</h2>
            <pre><code class="python" data-trim style="font-size: 1.3em">
high = uint32(unpack("=H", c[0:2])[0])
low = uint32(unpack("=H", c[2:4])[0])

hash += high
tmp = (low << uint32(11)) ^ hash
hash = (hash << uint32(16)) ^ tmp
hash += hash >> uint32(11)
            </code></pre>
            <aside class="notes">
              There are four different mixing functions for the 4, 3, 2 and 1
              byte cases, but let's focus on the 4 byte version. There are some
              gymnastics here around bitshifting, but note that the first and
              last shift amounts are the same.
            </aside>
          </section>

          <section>
            <h2>SuperFastHash</h2>
            <pre><code class="python" data-trim style="font-size: 1.3em">
hash ^= hash << uint32(3)
hash += hash >> uint32(5)
hash ^= hash << uint32(4)
hash += hash >> uint32(17)
hash ^= hash << uint32(25)
hash += hash >> uint32(6)

return hash
            </code></pre>
            <aside class="notes">
              Probably because of this, there's a lot of shifting in the post
              processing. The problem is that the damage has been done by now:
              the 4 byte loop often collapses into the same state, and then
              you're going to end up with basically the same output.
            </aside>
          </section>

          <section>
            <h2>SuperFastHash</h2>
            <table>
              <thead>
                <tr>
                  <th>Corpus</th>
                  <th>CPU time (s)</th>
                  <th>Collisions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numbers</td>
                  <td>2.195209</td>
                  <td>23946 (10.152%)</td>
                </tr>
                <tr>
                  <td>Words</td>
                  <td>2.609869</td>
                  <td>54 (0.023%)</td>
                </tr>
              </tbody>
            </table>
            <aside class="notes">
              Oh. Oh dear.
            </aside>
          </section>

          <section>
            <h2>SuperFastHash</h2>
            <img src="images/superfasthash.png">
            <aside class="notes">
              See that stripe down the right hand side? That's not good. That's
              a lot of values collapsing into the same buckets.
            </aside>
          </section>

          <section>
            <h2>JSHash</h2>
            <img src="images/jshash.png">
            <aside class="notes">
              Of course, it could be worse. I won't go through the
              implementation of this one, but this is the output of JSHash
              (Justin Sobel) with the numbers input.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1>CRC32</h1>
            <aside class="notes">
              Finally, there is the venerable CRC32. The Cyclic Redundancy
              Check family of functions rely on polynomial division to generate
              a checksum. There's serious maths here. I don't know if you're
              ready for the implementation...
            </aside>
          </section>

          <section>
            <h2>CRC32</h2>
            <pre><code class="python" data-trim style="font-size: 1.4em">
import zlib

def crc32(data: bytes) -> int:
    return zlib.crc32(data) & 0xffffffff
            </code></pre>
            <aside class="notes">
              (pause) So, why wouldn't you use this all the time? Actually, you
              might. It's not that bad.
            </aside>
          </section>

          <section>
            <h2>CRC32</h2>
            <table>
              <thead>
                <tr>
                  <th>Corpus</th>
                  <th>CPU time (s)</th>
                  <th>Collisions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numbers</td>
                  <td>0.576802</td>
                  <td>0</td>
                </tr>
                <tr>
                  <td>Words</td>
                  <td>0.572763</td>
                  <td>8 (0.003%)</td>
                </tr>
              </tbody>
            </table>
            <aside class="notes">
              Really, that's pretty good. The word collisions are potentially
              an issue, but it's still above average. The only other issue is
              memory usage: CRC32 uses a lookup table and keeps more in memory,
              but unless you're on the $7 Python computer, you should be fine.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h1 style="font-size: 3em">Conclusions</h1>
          </section>
          
          <section>
            <h2>Know your environment</h2>
            <aside class="notes">
              MurmurHash3 and xxHash smoke every other algorithm when
              implemented in (OK) C or C++, because they exploit how CPUs
              behave when you're writing at that level. They use cache locality
              and read alignment to their advantage. In Python, though, that's
              just overhead: the byte at a time approach is actually faster
              because of the way the bytes type is implemented, and other
              algorithms can be simpler.
            </aside>
          </section>

          <section>
            <h2>Know your requirements</h2>
            <aside class="notes">
              Understand how many collisions you're willing to bear. The best
              way to figure this out is to implement and test with sample data:
              hash functions are easy to implement, and you can test output
              using sets. If you can deal with a few more collisions, there are
              speedier options, like CRC32.
            </aside>
          </section>

          <section>
            <h2>Respect the standard library</h2>
            <aside class="notes">
              The C CRC32 implementation kills everything else, even though CRC32 isn't considered a fast algorithm in general. If you're calling hash functions a lot and don't want to write C yourself, use what Python gives you. It's free!
            </aside>
          </section>
        </section>

        <section>
          <h2>Thank you!</h2>
          <h3>Questions?</h3>
          <a href="https://twitter.com/LGnome">@LGnome</a>
        </section>
      </div>

		</div>

		<script src="reveal.js/lib/js/head.min.js"></script>
		<script src="reveal.js/js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: false,
				history: true,
				center: true,
        width: 1024,
        height: 768,

				transition: 'fade', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
<!-- vim: set nocin ai et ts=2 sw=2: -->
